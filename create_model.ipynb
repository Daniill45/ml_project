{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2202b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r req.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import psycopg2\n",
    "import os\n",
    "import datetime\n",
    "from dotenv import load_dotenv  \n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from category_encoders import TargetEncoder\n",
    "from category_encoders.one_hot import OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892ec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "display(HTML(\"<style>.container { font-size:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0892cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387cbdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подключаемся к бд(уже неактивно)\n",
    "post_text_df=pd.read_sql(\n",
    "    \"\"\"SELECT * FROM public.post_text_df \"\"\",\n",
    "    \n",
    "    con=\"postgresql://robot-startml-ro:pheiph0hahj1Vaif@\"\n",
    "        \"postgres.lab.karpov.courses:6432/startml\"\n",
    ")\n",
    "\n",
    "post_text_df # Информация о постах, которые будем рекомендовать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b9a5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data=pd.read_sql(\n",
    "    \"\"\"SELECT * FROM public.user_data \"\"\",\n",
    "    \n",
    "    con=\"postgresql://robot-startml-ro:pheiph0hahj1Vaif@\"\n",
    "        \"postgres.lab.karpov.courses:6432/startml\"\n",
    ")\n",
    "\n",
    "user_data # Информация о пользователях в бд"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a274a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_data = pd.read_sql(\n",
    "    \"\"\"SELECT * FROM public.feed_data LIMIT 100000\"\"\",\n",
    "    \n",
    "    con=\"postgresql://robot-startml-ro:pheiph0hahj1Vaif@\"\n",
    "        \"postgres.lab.karpov.courses:6432/startml\"\n",
    ")\n",
    "\n",
    "feed_data # Информация о действиях пользователей(просмотр/лайк)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce858732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список стран пользователей\n",
    "countries=list(user_data.country.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cd0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\sa-zA-Z0-9@\\[\\]]',' ',text) # Удаляет пунктцацию\n",
    "    text = re.sub(r'\\w*\\d+\\w*', '', text) # Удаляет цифры\n",
    "    text = re.sub(' ', \"\", text) # Удаляет ненужные пробелы\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117c6325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание датасета с выходными\n",
    "def create_holiday_dataset():\n",
    "    holiday=pd.read_csv(\".../yourpath/holiday_calendar.csv\")# Нужно добавить признак выходного дня, выходной либо выходной день недели, либо праздник\n",
    "    holiday.Date=pd.to_datetime(holiday.Date)\n",
    "    holiday[\"newdate\"]=holiday.Date.apply(lambda x:f\"2021-{x.month}-{x.day}\")\n",
    "    holiday[\"newdate\"]=pd.to_datetime(holiday.newdate)\n",
    "    holiday=holiday[holiday.Country.apply(lambda x:x.title() in countries)]\n",
    "    holiday=holiday[(holiday.Type=='National Holiday')]\n",
    "    holiday=holiday[[\"Country\",\"newdate\"]]\n",
    "    holiday=holiday[(holiday.newdate>=feed_data.timestamp.min())&(holiday.newdate<=feed_data.timestamp.max())]\n",
    "    return holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe4b7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dict():\n",
    "    holidays_dict={}\n",
    "    for i in holiday.values:\n",
    "        if i[0].title() not in holidays_dict:\n",
    "            a=i[0].title()\n",
    "            holidays_dict[a]=[i[1]]\n",
    "        else:\n",
    "            a=i[0].title()\n",
    "            holidays_dict[a].append(i[1])\n",
    "    holidays_dict['Finland']=[]\n",
    "    return holidays_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0330277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weeknd(new_table1):\n",
    "    holiday=create_holiday_dataset()\n",
    "    holidays_dict=create_dict()\n",
    "    new_table1[\"weekend\"]=0# Создаем признак выходного дня\n",
    "    for i in range(new_table1.shape[0]):\n",
    "        time=holidays_dict[new_table1.country[i]]\n",
    "        for j in time:\n",
    "            if j.month==new_table1.timestamp[i].month and j.day==new_table1.timestamp[i].day:\n",
    "                new_table1.weekend[i]=1 # Выходной день есть в датасете\n",
    "        if new_table1.day[i]==5 or new_table1.day[i]==6:\n",
    "            new_table1.weekend[i]=1 # Суббота или воскресенье\n",
    "    return new_table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f4bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обработаем feed_data\n",
    "def prepare_feed(feed_data):\n",
    "    feed_data=feed_data[feed_data.action==\"view\"]\n",
    "    feed_data=feed_data.drop(\"action\",axis=1)\n",
    "    feed_data[\"day_of_week\"]=pd.DatetimeIndex(feed_data['timestamp']).day_of_week # Признак дня недели\n",
    "    feed_data[\"month\"]=pd.DatetimeIndex(feed_data['timestamp']).month\n",
    "    feed_data[\"hour\"]=pd.DatetimeIndex(feed_data['timestamp']).hour\n",
    "    feed_data[\"day\"]=pd.DatetimeIndex(feed_data['timestamp']).day\n",
    "    return feed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data.city.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084fdb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ВВП по странам\n",
    "gdp={'Russia':32803, 'Ukraine':14220, 'Belarus':21699, 'Azerbaijan':15843, 'Kazakhstan':28600,\n",
    "       'Finland':53654, 'Turkey':30472, 'Latvia':34469, 'Cyprus':42556, 'Switzerland':77324, 'Estonia':42192}\n",
    "def city_exist(x,a):\n",
    "    if x in a:\n",
    "        return x\n",
    "    return \"no_city\"\n",
    "def prepare_user(user_data):\n",
    "    world_cities=pd.read_csv(\"/yourpath/worldcities.csv\") # Датасет городов мира\n",
    "    countries=list(user_data.country.unique())\n",
    "    world_cities=world_cities[world_cities.country.apply(lambda x: x in countries)] # Оставляем только нужные страны\n",
    "    world_cities=world_cities[[\"city\",\"lat\",\"lng\",\"country\",\"population\",\"admin_name\",\"capital\"]]\n",
    "    user_data.city=user_data.city.apply(lambda x:clean_text(x))\n",
    "    world_cities.city=world_cities.city.apply(lambda x:clean_text(x)) # Приводим названия городов к единому виду\n",
    "    a=set(world_cities.city)\n",
    "    user_data.city=user_data.city.apply(lambda x:city_exist(x,a)) # функция вовращает есть ли такой город в списке или нет\n",
    "    user_data=user_data[user_data.city!=\"no_city\"]\n",
    "    user_data=user_data.merge(world_cities, how='left', left_on='city', right_on='city').drop([\"country_y\"],axis=1)\n",
    "    user_data.admin_name.fillna(user_data.city,inplace=True)\n",
    "    user_data[\"is_capital\"]=0 # Живет ли человек в столице\n",
    "    user_data.loc[user_data.capital=='primary',\"is_capital\"]=1\n",
    "    user_data=user_data.drop([\"lat\",\"lng\",\"capital\"],axis=1)\n",
    "    user_data=user_data.rename({\"country_x\":\"country\"},axis=1)\n",
    "    user_data[\"parity_gdp\"]=user_data.country.apply(lambda x:gdp[x])\n",
    "    return user_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abe708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_post(post_text_df):\n",
    "\n",
    "\n",
    "    vectorizer = TfidfVectorizer() # Возьмем максимум и среднее tfidf как признак\n",
    "    ps=PorterStemmer() # Находим основу слова\n",
    "    words=[]\n",
    "    for word in post_text_df.text[0].split():\n",
    "        words.append(ps.stem(word))\n",
    "    text = vectorizer.fit_transform(post_text_df.text)\n",
    "    max_tfidf=[]\n",
    "    mean_tfidf=[]\n",
    "    tt=text.toarray()\n",
    "    for i in range(len(tt)):\n",
    "        mean_tfidf.append(np.mean(tt[i]))\n",
    "        max_tfidf.append(max(tt[i]))\n",
    "    post_text_df[\"mean_tf_idf_2\"]=mean_tfidf\n",
    "    post_text_df[\"max_tf_idf_2\"]=max_tfidf\n",
    "    lenght=[]\n",
    "    for i in post_text_df.text:\n",
    "        lenght.append(len(i))\n",
    "    post_text_df[\"lenght_text\"]=lenght\n",
    "    #post_text_df_2=post_text_df.drop([\"text\"],axis=1)\n",
    "    \n",
    "    return post_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5542ca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим таблицу\n",
    "def create_table():\n",
    "    global post_text_df,feed_data,user_data\n",
    "    feed_data=prepare_feed(feed_data)\n",
    "    print(\"feed_ready\")\n",
    "    user_data=prepare_user(user_data)\n",
    "    print(\"user_ready\")\n",
    "    post_text_df=prepare_post(post_text_df)\n",
    "    print(\"post_ready\")\n",
    "    new_table=user_data.merge(feed_data, how='left', left_on='user_id', right_on='user_id')\n",
    "    new_table=new_table.merge(post_text_df, how='left', left_on='post_id', right_on='post_id')\n",
    "    #new_table=new_table[~new_table.post_id.isna()]\n",
    "    return new_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af9f644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Напишем кастомный трансформер, чтобы он автоматически применял mean target encoding/one hot encoding\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import itertools\n",
    "\n",
    "class CustomFunctionTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 object_columns=[],\n",
    "                 target_name='target'):\n",
    "        \n",
    "        self.object_columns = object_columns\n",
    "        self.target_name = target_name\n",
    "        \n",
    "                \n",
    "    def fit(self,\n",
    "            X,\n",
    "            y):\n",
    "        \n",
    "        X_fit = X.copy()\n",
    "        y_fit = y.copy()\n",
    "\n",
    "        self.numeric_columns = [x for x in X_fit.columns if x not in self.object_columns]\n",
    "        \n",
    "        X_with_target = pd.concat((X_fit, y_fit), axis=1)\n",
    "        \n",
    "        # Сгенерим колонки к которым применим One-Hot-Encoding\n",
    "        self.cols_for_ohe = [col for col in self.object_columns\n",
    "                             if \n",
    "                             X_with_target[col].nunique() <= 13]\n",
    "        \n",
    "        # Запомним все ohe колонки и их названия!\n",
    "        self.ohe_names = {col : sorted([f\"{col}_{value}\" for value in X_with_target[col].unique()])\n",
    "                          for col in self.cols_for_ohe}\n",
    "        \n",
    "        \n",
    "        # Сгенерим колонки к которым применим Mean-Target-Encoding\n",
    "        self.cols_for_mte = [col for col in self.object_columns\n",
    "                             if X_with_target[col].nunique() > 13]\n",
    "        \n",
    "        # Посчитаем на валидации средние значения таргета+ шум\n",
    "        self.dict_of_means = {col : X_with_target.groupby(col)[self.target_name].mean() + 0.006*np.random.randn() \n",
    "                              for col in self.cols_for_mte}\n",
    "        \n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self,\n",
    "                  X,\n",
    "                  y=None):\n",
    "        \n",
    "        X_ = X.copy()\n",
    "        \n",
    "        \n",
    "        data_part = pd.get_dummies(X_[self.cols_for_ohe],\n",
    "                                   prefix=self.cols_for_ohe)\n",
    "        \n",
    "        data_part_cols = data_part.columns\n",
    "        \n",
    "        X_ = X_.drop(self.cols_for_ohe, axis=1)\n",
    "        X_ = pd.concat((X_, data_part), axis=1)\n",
    "        \n",
    "    \n",
    "        for col in self.cols_for_mte:\n",
    "                X_[col] = X_[col].map(self.dict_of_means[col])\n",
    "                \n",
    "                mean_value = self.dict_of_means[col].values.mean()\n",
    "                \n",
    "                X_[col] = X_[col].fillna(mean_value)# заполняем значения которых нет в тесте средним\n",
    "                \n",
    "            \n",
    "            \n",
    "        all_ohe = list(itertools.chain(*list(self.ohe_names.values())))\n",
    "        \n",
    "        missing_columns = [x \n",
    "                           for x in all_ohe\n",
    "                           if x not in X_.columns\n",
    "                           and\n",
    "                           x not in self.numeric_columns]\n",
    "\n",
    "        extra_columns = [x\n",
    "                         for x in data_part_cols\n",
    "                         if x not in all_ohe]\n",
    "        \n",
    "        # Новые категории необходимо убрать\n",
    "        X_ = X_.drop(extra_columns, axis=1)\n",
    "    \n",
    "        # Отсутствующие категории (бинарные колонки)\n",
    "        # необходимо добавить: заполним их просто нулями\n",
    "        \n",
    "        if len(missing_columns) != 0:\n",
    "\n",
    "            zeros = np.zeros((X_.shape[0], len(missing_columns)))\n",
    "            zeros = pd.DataFrame(zeros,\n",
    "                                 columns=missing_columns,\n",
    "                                 index=X_.index)\n",
    "\n",
    "            X_ = pd.concat((X_, zeros), axis=1)\n",
    "            \n",
    "        return X_[sorted(X_.columns)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8a221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_post_text=pd.read_csv(\"/yourpath/embeddings_post_text\",index_col=0) # Эмбеддинги для текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2a8345",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_text_df=pd.concat([embeddings_post_text,post_text_df.text],axis=1) # Соединяем эмбединги и изначальные данные о постах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaef64a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_table1=create_table() # Создаем таблицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae150351",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59997f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical=list(new_table1.loc[:,new_table1.dtypes==object].columns)\n",
    "numeric=list(new_table1.loc[:,new_table1.dtypes!=object].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539201a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table1[numeric] = new_table1[numeric].fillna(new_table1[numeric].median()) # заполняем медианой вещественные признаки\n",
    "new_table1[categorical] = new_table1[categorical].fillna(new_table1[categorical].mode().iloc[0]) # заполняем самым популярным значением кат-ые признаки \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d50155",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table1=new_table1.drop([\"timestamp\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2631ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=new_table1.target\n",
    "X=new_table1.drop([\"target\"],axis=1)\n",
    "categorical=list(X.loc[:,new_table1.dtypes==object].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad8e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"custom_transformer\",\n",
    "                  CustomFunctionTransformer(categorical,target_name=\"target\")),\n",
    "                 (\"cat\", \n",
    "                     CatBoostClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0a2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\"iterations\":[200],\"learning_rate\":[0.1],\"depth\":[7]} # Выбираем лучшие прааметры(оставил только оптимальные)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2151c83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c3b6e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'Качество модели на трейне: {roc_auc_score(y_train, pipe.predict_proba(X_train)[:, 1])}')\n",
    "print(f'Качество модели на тесте: {roc_auc_score(y_test, pipe.predict_proba(X_test)[:, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d92972",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat=CatBoostClassifier(random_seed=1,iterations=200,learning_rate=0.1,depth=7,cat_features=categorical,ignored_features=[\"post_id\",\"user_id\",\"source\",\"os\",\"text\"])\n",
    "cat.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c530f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Смотрим на roc_auc, потому что нам важно правильно оценивать вероятность того, что пользователь лайкнет пост\n",
    "print(f'Качество модели на трейне: {roc_auc_score(y_train, cat.predict_proba(X_train)[:, 1])}') \n",
    "print(f'Качество модели на тесте: {roc_auc_score(y_test, cat.predict_proba(X_test)[:, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1311259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc0f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_feature_importance(importance,names,model_type):\n",
    "    \n",
    "    #Create arrays from feature importance and feature names\n",
    "    feature_importance = np.array(importance)\n",
    "    feature_names = np.array(names)\n",
    "    \n",
    "    #Create a DataFrame using a Dictionary\n",
    "    data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "    fi_df = pd.DataFrame(data)\n",
    "    \n",
    "    #Sort the DataFrame in order decreasing feature importance\n",
    "    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
    "    \n",
    "    #Define size of bar plot\n",
    "    plt.figure(figsize=(10,8))\n",
    "    #Plot Searborn bar chart\n",
    "    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n",
    "    #Add chart labels\n",
    "    plt.title(model_type + 'FEATURE IMPORTANCE')\n",
    "    plt.xlabel('FEATURE IMPORTANCE')\n",
    "    plt.ylabel('FEATURE NAMES')\n",
    "    \n",
    "plot_feature_importance(cat.feature_importances_,X_train.columns,'Catboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b471fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.save_model('11catboost_model',\n",
    "                           format=\"cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199c2ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_info.to_sql(    \n",
    "   \"posts_info_features\",                    \n",
    "    con=\"postgresql://username:password@host:port/database\",                      \n",
    "    schema=\"public\",                   \n",
    "    if_exists='replace'            \n",
    "   )                               \n",
    "                                   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
